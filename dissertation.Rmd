---
title: "Contextual Trust Study"
output:
  html_document:
    code_download: yes
    fontsize: 8pt
    highlight: textmate
    number_sections: no
    theme: cosmo
    toc: yes
    toc_float:
      collapsed: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning =  FALSE)
knitr::opts_chunk$set(fig.width=3.25)
knitr::opts_chunk$set(fig.height=2.75)
knitr::opts_chunk$set(fig.align='center') 
knitr::opts_chunk$set(results='hold') 
```

```{r, echo=FALSE, warning=FALSE}
library(psych)
library(lme4)
library(plyr)
library(tidyverse)
library(ggplot2)
library(ggstatsplot)
library(data.table)
library(gmodels)
library(texreg)
library(lmerTest)
library(apaTables)

```

```{r, echo=FALSE, warning=FALSE}
data1<-read.csv("Dissertation_IdioTrust.csv", header = T, na.strings = c(""))
```


# Brief Rationale of my study
Interpersonal trust is an important facet of relationships, but current theories and research paradigms on interpersonal trust may be incomplete. Trust has traditionally been studied as either a trait of the individual or a facet of a specific relationship between two people. Although both approaches highlight different and important aspects of the psychology of trust, both approaches are limited in their ability to uncover more minute dynamics. 

In my dissertation, I proposed and tested a goal-specific theory of interpersonal trust, the Interdependent Goal Model of Interpersonal Trust (IGM). I argued that the trust between two people varies across the interdependent goals that are shared within a relationship and that this variability is important for understanding interpersonal relationships. 

Testing the hypotheses derived from the IGM required a methodology sensitive to the idiosyncratic goals and relationships of the participants. To this end, I employed a novel and ideographically-tailored survey in which participants described their current goals and best friend. After participants described their goals and best friend, they were asked to what extent they would trust their best friend across some of their most and least important goals.  

On this page, I will walk you through the logic and main findings of my dissertation, including all necessary data pre-processing steps and my analyses. If you're interested in a more detailed account of my theory, hypotheses, and methods, then I encourage you to read my dissertation [here](Carsel_2020_Interdependent goal model of trust.pdf)
<br/><br/><br/><br/>




# Hypotheses
- The first hypothesis of the present study is that interpersonal trust will increase as the relevance of a best friend’s strength to a given task increases, and interpersonal trust will decrease as the relevance of a best friend’s weakness to a given task increases. 

- The second hypothesis of the present study is that individuals will place lower trust in another person on average across a range of highly important goals relative to goals of lower importance.

- The third hypothesis of the present study is that for more important goals, people will be more likely to trust an important relational partner (e.g., best friend) if they believe the trustee’s strengths are relevant to the task and less likely to trust that relational partner if they believe the trustee’s weaknesses are relevant to the task. For goals that are less important, beliefs about the trustee’s strengths and weaknesses will be less relevant.
<br/><br/><br/><br/>




# Survey Methodology Overview
In order to understand any analysis, it's important to first understand the data generation process. In the case of these data, participants completed an idiographically-tailored survey. 

A traditional survey uses what's called a **nomothetic** approach, which aims to understand how groups of people act. These are the kinds of surveys people take all the time. For example, if you've ever completed a customer satisfaction survey that asked you questions like "On a scale of 1 to 5, how satisfied were you with the service you received?", then you have completed a nomothetically-designed survey. 

By contrast, an **idiographic** approach aims to understand the idiosyncracies of the individual. An example of an idiographically-designed customer satisfaction survey question might be something more along the lines of "What was the most important part of the service you received for why you gave the rating you did?", with an open-ended text box for people to write in their response.
<br/><br/><br/>

## Method
In idiographically-tailored survey from which we got our data, participants were asked about their goals and their best friend. Specifically, participants first listed the goals they were currently pursuing, and then they were asked to select their three most and three least important goals. Next, participants were asked to identify their best friend and their best friend's most descriptive strength and weakness. Afterwards, participants were asked to what extent their best friend's most descriptive strength and weakness were relevant to each of the participants' three most and three least important goals. Finally, participants were asked to what extent they would trust their best friend across their three most and three least important goals. The criterion variable of this study, or the thing we want to predict, is the trust participants placed in their best friend across those six goals.
<br/><br/><br/><br/>




# Data Pre-processing
The first step in analyzing data in ```R``` is to set your working environment and then load your data and required packages. I have already done that in the background, so I encourage you to download the code for this page if you would like to see that step. I will only show one example of code for each pre-processing step to save precious internet space.

After you've set up your enviornment you can start pre-processing your data to get it into the necessary format for analyses. Below, I create a participant identification number (PIN) and set it as a factor. Then I rename the variables that identify exactly *how* important, on a 1 to 10 scale, participants viewed their three most and three least important goals.

```{r, warning = FALSE}
# Creating a PIN
data1$PIN<-seq(from=1, to=length(data1$Progress))
# Setting the variable as a factor for analyses
data1$PIN<-as.factor(data1$PIN)


## Creating variables for the continuous measure of goal importance 
# Most Important Goals 
data1$g1Imp<-(as.numeric(data1$Q5.2)-1)
```

```{r, warning = FALSE, echo = FALSE}
data1$g2Imp<-(as.numeric(data1$Q6.2)-1)
data1$g3Imp<-(as.numeric(data1$Q7.2)-1)
# Least Important Goals
data1$g4Imp<-(as.numeric(data1$Q8.2)-1)
data1$g5Imp<-(as.numeric(data1$Q9.2)-1)
data1$g6Imp<-(as.numeric(data1$Q10.2)-1)

```

Next, I have to create the variables that indicate to what extent participants' best friend's most descriptive strength and weakness were to each of the six goals. Here is where we come across our first coding challenge.

Because this survey was idiographically-tailored, and participants could have identified anywhere from 6 to 25 goals, these data are what's known as **sparse**. Sparse data has a lot of empty cells, and this was one of the sparsest data sets I've ever worked with. In this case, there was a single value for every participant scattered across 33 columns! I mean, take a look at this hot mess!

```{r}
head(data1[, 1311:1345])
```

Here is the code I used to condense that sparse data into a set of variables so that every participants' value was included in a single variable.

```{r, warning = FALSE}
### Creating Goal Relevance Variables 
## Most Important Goals ##########
# Goal 1 Strength
data1$Goal1StrRel<-(apply(data1[, 1311:1345], 1, function(x) x[!is.na(x)][1]))-1
```

```{r, warning = FALSE, echo = FALSE}
# Goal 1 Weakness
data1$Goal1WeakRel<-(apply(data1[, 1346:1380], 1, function(x) x[!is.na(x)][1]))-1
# Goal 2 Strength
data1$Goal2StrRel<-(apply(data1[, 1381:1415], 1, function(x) x[!is.na(x)][1]))-1
# Goal 2 Weakness
data1$Goal2WeakRel<-(apply(data1[, 1416:1450], 1, function(x) x[!is.na(x)][1]))-1
# Goal 3 Strength
data1$Goal3StrRel<-(apply(data1[, 1451:1485], 1, function(x) x[!is.na(x)][1]))-1
# Goal 3 Weakness
data1$Goal3WeakRel<-(apply(data1[, 1486:1520], 1, function(x) x[!is.na(x)][1]))-1
## Least Important Goals
# Goal 4 Strength
data1$Goal4StrRel<-(apply(data1[, 1521:1555], 1, function(x) x[!is.na(x)][1]))-1
# Goal 4 Weakness
data1$Goal4WeakRel<-(apply(data1[, 1556:1590], 1, function(x) x[!is.na(x)][1]))-1
# Goal 5 Strength
data1$Goal5StrRel<-(apply(data1[, 1591:1625], 1, function(x) x[!is.na(x)][1]))-1
# Goal 5 Weakness
data1$Goal5WeakRel<-(apply(data1[, 1626:1660], 1, function(x) x[!is.na(x)][1]))-1
# Goal 6 Strength
data1$Goal6StrRel<-(apply(data1[, 1661:1695], 1, function(x) x[!is.na(x)][1]))-1
# Goal 6 Weakness
data1$Goal6WeakRel<-(apply(data1[, 1696:1730], 1, function(x) x[!is.na(x)][1]))-1
```

Let's see if that worked.

```{r}
head(data1$Goal1StrRel:data1$Goal6StrRel)
```

Great! Now we have condensed our first set of predictors, and it's time to move onto the next set of variables.

The variables that contained participants' trust in their best friend across each goal was similarly sparse, so I simply applied the same code to fix that here. However, the measures for goal-specific trust were a bit more complicated, so they require more code. Specifically, participants were first asked if they trusted their best friend or not for each goal, and then they were asked to what extent they trusted or distrusted their best friend, depending on whether they first selcted trust or distrust. Because we was a continuous measure for our analyses, we need to somehow combine those extent variables with the dichotomous measure. Below is the code for how I accomplished that. 

```{r, warning = FALSE}
### Creating Goal-Based Trust DVs ###
## Trust for Project 1 
# Trust
data1$Goal1TrustDi<-(apply(data1[, 1731:1765], 1, function(x) x[!is.na(x)][1]))
# Extent Ss Trusts Best Friend for Project 1
data1$G1TrustExt<-as.numeric(data1$Q26.2)
# Extent Ss distrusts Best Friend for Project 1
data1$G1DistrustExt<-as.numeric(-data1$Q26.3)
# Lean Toward Trust vs Distrust
data1$G1TrustLean<-as.numeric(data1$Q26.4)
# Creating bipolar measure of Trust-Distrust
data1$Goal1TrustBP<-ifelse(data1$Goal1TrustDi==1,data1$G1TrustExt,
                           ifelse(data1$Goal1TrustDi==2,data1$G1DistrustExt,
                           ifelse(data1$G1TrustLean==1,1,
                           ifelse(data1$G1TrustLean==2,-1,NA))))

# Rely
data1$Goal1RelyDi<-(apply(data1[, 1769:1803], 1, function(x) x[!is.na(x)][1]))
# Extent Ss Trusts Best Friend for Project 1
data1$G1RelyExt<-as.numeric(data1$Q26.6)
# Extent Ss distrusts Best Friend for Project 1
data1$G1UnrelyDiExt<-as.numeric(-data1$Q26.7)
# Lean Toward Trust vs Distrust
data1$G1RelyLean<-as.numeric(data1$Q26.8)
# Creating bipolar measure of Trust-Distrust
data1$Goal1RelyBP<-ifelse(data1$Goal1RelyDi==1,data1$G1RelyExt,
                           ifelse(data1$Goal1RelyDi==2,data1$G1UnrelyDiExt,
                           ifelse(data1$G1RelyLean==1,1,
                           ifelse(data1$G1RelyLean==2,-1,NA))))
```

```{r, warning = FALSE, echo = FALSE}
## Trust for Project 2 
# Trust
data1$Goal2TrustDi<-(apply(data1[, 1807:1841], 1, function(x) x[!is.na(x)][1]))
# Extent Ss Trusts Best Friend for Project 1
data1$G2TrustExt<-as.numeric(data1$Q27.2)
# Extent Ss distrusts Best Friend for Project 1
data1$G2DistrustExt<-as.numeric(-data1$Q27.3)
# Lean Toward Trust vs Distrust
data1$G2TrustLean<-as.numeric(data1$Q27.4)
# Creating bipolar measure of Trust-Distrust
data1$Goal2TrustBP<-ifelse(data1$Goal2TrustDi==1,data1$G2TrustExt,
                           ifelse(data1$Goal2TrustDi==2,data1$G2DistrustExt,
                           ifelse(data1$G2TrustLean==1,1,
                           ifelse(data1$G2TrustLean==2,-1,NA))))

# Rely
data1$Goal2RelyDi<-(apply(data1[, 1845:1879], 1, function(x) x[!is.na(x)][1]))
# Extent Ss Trusts Best Friend for Project 1
data1$G2RelyExt<-as.numeric(data1$Q27.6)
# Extent Ss distrusts Best Friend for Project 1
data1$G2UnrelyDiExt<-as.numeric(-data1$Q27.7)
# Lean Toward Trust vs Distrust
data1$G2RelyLean<-as.numeric(data1$Q27.8)
# Creating bipolar measure of Trust-Distrust
data1$Goal2RelyBP<-ifelse(data1$Goal2RelyDi==1,data1$G2RelyExt,
                          ifelse(data1$Goal2RelyDi==2,data1$G2UnrelyDiExt,
                          ifelse(data1$G2RelyLean==1,1,
                          ifelse(data1$G2RelyLean==2,-1,NA))))

## Trust for Project 3 
# Trust
data1$Goal3TrustDi<-(apply(data1[, 1883:1917], 1, function(x) x[!is.na(x)][1]))
# Extent Ss Trusts Best Friend for Project 1
data1$G3TrustExt<-as.numeric(data1$Q28.2)
# Extent Ss distrusts Best Friend for Project 1
data1$G3DistrustExt<-as.numeric(-data1$Q28.3)
# Lean Toward Trust vs Distrust
data1$G3TrustLean<-as.numeric(data1$Q28.4)
# Creating bipolar measure of Trust-Distrust
data1$Goal3TrustBP<-ifelse(data1$Goal3TrustDi==1,data1$G3TrustExt,
                           ifelse(data1$Goal3TrustDi==2,data1$G3DistrustExt,
                           ifelse(data1$G3TrustLean==1,1,
                           ifelse(data1$G3TrustLean==2,-1,NA))))

# Rely
data1$Goal3RelyDi<-(apply(data1[, 1921:1955], 1, function(x) x[!is.na(x)][1]))
# Extent Ss Trusts Best Friend for Project 1
data1$G3RelyExt<-as.numeric(data1$Q28.6)
# Extent Ss distrusts Best Friend for Project 1
data1$G3UnrelyDiExt<-as.numeric(-data1$Q28.7)
# Lean Toward Trust vs Distrust
data1$G3RelyLean<-as.numeric(data1$Q28.8)
# Creating bipolar measure of Trust-Distrust
data1$Goal3RelyBP<-ifelse(data1$Goal3RelyDi==1,data1$G3RelyExt,
                          ifelse(data1$Goal3RelyDi==2,data1$G3UnrelyDiExt,
                          ifelse(data1$G3RelyLean==1,1,
                          ifelse(data1$G3RelyLean==2,-1,NA))))

## Trust for Project 4 
# Trust
data1$Goal4TrustDi<-(apply(data1[, 1959:1993], 1, function(x) x[!is.na(x)][1]))
# Extent Ss Trusts Best Friend for Project 1
data1$G4TrustExt<-as.numeric(data1$Q29.2)
# Extent Ss distrusts Best Friend for Project 1
data1$G4DistrustExt<-as.numeric(-data1$Q29.3)
# Lean Toward Trust vs Distrust
data1$G4TrustLean<-as.numeric(data1$Q29.4)
# Creating bipolar measure of Trust-Distrust
data1$Goal4TrustBP<-ifelse(data1$Goal4TrustDi==1,data1$G4TrustExt,
                           ifelse(data1$Goal4TrustDi==2,data1$G4DistrustExt,
                           ifelse(data1$G4TrustLean==1,1,
                           ifelse(data1$G4TrustLean==2,-1,NA))))

# Rely
data1$Goal4RelyDi<-(apply(data1[, 1997:2031], 1, function(x) x[!is.na(x)][1]))
# Extent Ss Trusts Best Friend for Project 1
data1$G4RelyExt<-as.numeric(data1$Q29.6)
# Extent Ss distrusts Best Friend for Project 1
data1$G4UnrelyDiExt<-as.numeric(-data1$Q29.7)
# Lean Toward Trust vs Distrust
data1$G4RelyLean<-as.numeric(data1$Q29.8)
# Creating bipolar measure of Trust-Distrust
data1$Goal4RelyBP<-ifelse(data1$Goal4RelyDi==1,data1$G4RelyExt,
                          ifelse(data1$Goal4RelyDi==2,data1$G4UnrelyDiExt,
                          ifelse(data1$G4RelyLean==1,1,
                          ifelse(data1$G4RelyLean==2,-1,NA))))

## Trust for Project 5 
# Trust
data1$Goal5TrustDi<-(apply(data1[, 2035:2069], 1, function(x) x[!is.na(x)][1]))
# Extent Ss Trusts Best Friend for Project 1
data1$G5TrustExt<-as.numeric(data1$Q30.2)
# Extent Ss distrusts Best Friend for Project 1
data1$G5DistrustExt<-as.numeric(-data1$Q30.3)
# Lean Toward Trust vs Distrust
data1$G5TrustLean<-as.numeric(data1$Q30.4)
# Creating bipolar measure of Trust-Distrust
data1$Goal5TrustBP<-ifelse(data1$Goal5TrustDi==1,data1$G5TrustExt,
                           ifelse(data1$Goal5TrustDi==2,data1$G5DistrustExt,
                           ifelse(data1$G5TrustLean==1,1,
                           ifelse(data1$G5TrustLean==2,-1,NA))))

# Rely
data1$Goal5RelyDi<-(apply(data1[, 2073:2107], 1, function(x) x[!is.na(x)][1]))
# Extent Ss Trusts Best Friend for Project 1
data1$G5RelyExt<-as.numeric(data1$Q30.6)
# Extent Ss distrusts Best Friend for Project 1
data1$G5UnrelyDiExt<-as.numeric(-data1$Q30.7)
# Lean Toward Trust vs Distrust
data1$G5RelyLean<-as.numeric(data1$Q30.8)
# Creating bipolar measure of Trust-Distrust
data1$Goal5RelyBP<-ifelse(data1$Goal5RelyDi==1,data1$G5RelyExt,
                          ifelse(data1$Goal5RelyDi==2,data1$G5UnrelyDiExt,
                          ifelse(data1$G5RelyLean==1,1,
                          ifelse(data1$G5RelyLean==2,-1,NA))))

## Trust for Project 6 
# Trust
data1$Goal6TrustDi<-(apply(data1[, 2111:2145], 1, function(x) x[!is.na(x)][1]))
# Extent Ss Trusts Best Friend for Project 1
data1$G6TrustExt<-as.numeric(data1$Q31.2)
# Extent Ss distrusts Best Friend for Project 1
data1$G6DistrustExt<-as.numeric(-data1$Q31.3)
# Lean Toward Trust vs Distrust
data1$G6TrustLean<-as.numeric(data1$Q31.4)
# Creating bipolar measure of Trust-Distrust
data1$Goal6TrustBP<-ifelse(data1$Goal6TrustDi==1,data1$G6TrustExt,
                           ifelse(data1$Goal6TrustDi==2,data1$G6DistrustExt,
                           ifelse(data1$G6TrustLean==1,1,
                           ifelse(data1$G6TrustLean==2,-1,NA))))

# Rely
data1$Goal6RelyDi<-(apply(data1[, 2149:2183], 1, function(x) x[!is.na(x)][1]))
# Extent Ss Trusts Best Friend for Project 1
data1$G6RelyExt<-as.numeric(data1$Q31.6)
# Extent Ss distrusts Best Friend for Project 1
data1$G6UnrelyDiExt<-as.numeric(-data1$Q31.7)
# Lean Toward Trust vs Distrust
data1$G6RelyLean<-as.numeric(data1$Q31.8)
# Creating bipolar measure of Trust-Distrust
data1$Goal6RelyBP<-ifelse(data1$Goal6RelyDi==1,data1$G6RelyExt,
                          ifelse(data1$Goal6RelyDi==2,data1$G6UnrelyDiExt,
                          ifelse(data1$G6RelyLean==1,1,
                          ifelse(data1$G6RelyLean==2,-1,NA))))
```

Whew!! That's a lot! But we're almost done. We have created our PIN, the continuous measure of goal mportance (for a manipulation check), the relevance of the best friend's most descriptive strength and weakness to each goal, and the measure of goal-specific trust. All that is left is to create a subset of the data with only the variables that we need and to put it into the proper format.

Here, I subset the data and create the dataframme that I'll use for plotting and my analyses

```{r, warning = FALSE}
data2<-subset(data1,select=c(PIN:Goal1TrustDi,Goal1TrustBP,Goal1RelyDi,Goal1RelyBP,Goal2TrustDi,
                             Goal2TrustBP,Goal2RelyDi,Goal2RelyBP,Goal3TrustDi,Goal3TrustBP,Goal3RelyDi,
                             Goal3RelyBP,Goal4TrustDi,Goal4TrustBP,Goal4RelyDi,Goal4RelyBP,Goal5TrustDi,
                             Goal5TrustBP,Goal5RelyDi,Goal5RelyBP,Goal6TrustDi,Goal6TrustBP,Goal6RelyDi,
                             Goal6RelyBP)) 
```

Let's check it to make sure everything worked properly

```{r, warning = FALSE}
str(data2)
```

Perfect! Now, let's arrange the variables for easier manipulation and set better names. In order to do this, we'll need to convert our data from **wide format** to **long format**.

Most people are probably more familiar with **wide format**. In **wide format**, every column is a variable, and every row is a case or instance of the variable "participant." In other words, each case represents a single participant. In **long format**, we stretch out our data so that each participant occupies multimple rows. In the end of this transformation, each row will be an instance or case of the variable "goal" so that every participant will occupy **6** rows, one for each of their three most and three least important goals.

```{r, warning = FALSE}
### Arranging columns for easier manipulation 
## Goal Importance 
# Select vars
impData<-data2 %>% dplyr::select(tidyselect::vars_select(names(data2), dplyr::matches('Imp')))
# Add PIN
impData$PIN<-data2$PIN
# Wide to Long
impLong<-impData %>% gather(Goal,Importance,g1Imp:g6Imp)
# Rename factor levels
impLong$Goal<-mapvalues(impLong$Goal, from = c("g1Imp","g2Imp","g3Imp","g4Imp","g5Imp","g6Imp"),
                        to = c("Goal1","Goal2","Goal3","Goal4","Goal5","Goal6"))

## Relevance of strength to each project 
# Select vars
strengthData<-data2 %>% dplyr::select(tidyselect::vars_select(names(data2), matches('StrRel')))
# Add PIN
strengthData$PIN<-data2$PIN
# Wide to Long
srelLong<-strengthData %>% gather(Goal,StRel,Goal1StrRel:Goal6StrRel)
# Rename factor levels
srelLong$Goal<-mapvalues(srelLong$Goal, from = c("Goal1StrRel","Goal2StrRel","Goal3StrRel","Goal4StrRel","Goal5StrRel","Goal6StrRel"),
                         to = c("Goal1","Goal2","Goal3","Goal4","Goal5","Goal6"))

## Relevance of weakness to each project 
# Select vars
weakData<-data2 %>% dplyr::select(tidyselect::vars_select(names(data2), matches('WeakRel')))
# Add PIN
weakData$PIN<-data2$PIN
# Wide to Long
wrelLong<-weakData %>% gather(Goal,WeakRel,Goal1WeakRel:Goal6WeakRel)
# Rename factor levels
wrelLong$Goal<-mapvalues(wrelLong$Goal, from = c("Goal1WeakRel","Goal2WeakRel","Goal3WeakRel","Goal4WeakRel","Goal5WeakRel","Goal6WeakRel"),
                         to = c("Goal1","Goal2","Goal3","Goal4","Goal5","Goal6"))

## Trust best friend 
# Select vars
trustData<-data2 %>% dplyr::select(tidyselect::vars_select(names(data2), matches('TrustBP')))
# Add PIN
trustData$PIN<-data2$PIN
# Wide to Long
tLong<-trustData %>% gather(Goal,Trust,Goal1TrustBP:Goal6TrustBP)
# Rename factor levels
tLong$Goal<-mapvalues(tLong$Goal, from = c("Goal1TrustBP","Goal2TrustBP","Goal3TrustBP","Goal4TrustBP","Goal5TrustBP","Goal6TrustBP"),
                      to = c("Goal1","Goal2","Goal3","Goal4","Goal5","Goal6"))

## Rely on best friend 
# Select vars
relyData<-data2 %>% dplyr::select(tidyselect::vars_select(names(data2), matches('RelyBP')))
# Add PIN
relyData$PIN<-data2$PIN
# Wide to Long
rLong<-relyData %>% gather(Goal,Rely,Goal1RelyBP:Goal6RelyBP)
# Rename factor levels
rLong$Goal<-mapvalues(rLong$Goal, from = c("Goal1RelyBP","Goal2RelyBP","Goal3RelyBP","Goal4RelyBP","Goal5RelyBP","Goal6RelyBP"),
                      to = c("Goal1","Goal2","Goal3","Goal4","Goal5","Goal6"))
```

Now that we've created several smaller dataframes in **long format**, let's merge them together to create a dataframe that's in **long format** and has all of our variables. While we're doing this, we will create a categorical variable that indicates whether a goal is one of the three most or three least important goals. 

```{r, warning = FALSE}
### Creating Working Long Data
# Importance and Trust
imp.trust<-merge(tLong,impLong,by=c("Goal","PIN"))
# Adding rely
imp.trust.rely<-merge(imp.trust,rLong,by=c("Goal","PIN"))
# Adding relevance to strength
imp.trust.rely.str<-merge(imp.trust.rely,srelLong,by=c("Goal","PIN"))
# Adding relevance to weakness
data3<-merge(imp.trust.rely.str,wrelLong,by=c("Goal","PIN"))

# Converting goal variable to factor for analyses
data3$Goal<-as.factor(data3$Goal)

# Adding Factor Indicating Most vs Least Important Projects
data3$impCat<-ifelse(data3$Goal=="Goal1", 1,
              ifelse(data3$Goal=="Goal2", 1,
              ifelse(data3$Goal=="Goal3", 1,
              ifelse(data3$Goal=="Goal4", 0,
              ifelse(data3$Goal=="Goal5", 0,
              ifelse(data3$Goal=="Goal6", 0,NA))))))
# Converting to Factor
data3$impCat<-as.factor(data3$impCat)

data3$impCat<-factor(data3$impCat,
                     levels=c(0,1),
                     labels=c("Least Important","Most Important"))
```

Now let's check our new dataframe to make sure all our variables are there and that they're in the proper format.

```{r, warning = FALSE}
str(data3)

head(data3)

tail(data3)
```

Now that we have the data in the appropriate format, we only have a couple more pre-processing steps. First, we need to center our predictor variables.

There are two ways I can choose to center my predictor variables, by **participant** or **globally**. **Globally-centered** variables will subtract the **grand mean**, or overall average of the variable, from each participant's value. The end result will be that the average value of that variable across all participants will be 0. **Participant-centered** variables subtract *each participant's* mean from each of their values. The end result of *this* process will be that the average for each participant on each variable is 0. Each of these centering strategies tells a different story, so it's important to choose whichever one makes the most sense for your research question. In this case, **participant-centered** variables makes the most sense. So that's what we're going to do!

```{r, warning = FALSE}
## Centering IVs by Ss 
# Relevance to Strength
# Mean of Relevance to Strength
data3<-ddply(data3,.(PIN), plyr::mutate, rStrMean = mean(StRel))
# Participant-centered Relevance to Strength
data3$StrRelC<-data3$StRel-data3$rStrMean
# Globally-centered Relevance to Strength
data3$StrRel_GlobC <- scale(data3$StRel, scale = FALSE)[,]
```

```{r, warning = FALSE, echo = FALSE}
# Relevance to Weakness
data3<-ddply(data3,.(PIN), plyr::mutate, rWeakMean = mean(WeakRel))
data3$WeakRelC<-data3$WeakRel-data3$rWeakMean
data3$WeakRel_GlobC <- scale(data3$WeakRel, scale = FALSE)[,]

# Importance
data3<-ddply(data3,.(PIN), plyr::mutate, ImpC = mean(Importance))
data3$ImportanceC <-data3$Importance-data3$ImpC
data3$Imp_GlobC <- scale(data3$Importance, scale = FALSE)[,]

```

Now that we've centered our predictors, let's check whether our two questions that are to make up our composite measure of goal-specific trust are sufficiently correlated. In other words, I want to average the responses to the question "To what extent would you *trust* your best friend" with the responses to the question "To what extent would you be willing to *rely* on your best friend?" However, if these items do not actually measure the same thing (i.e., the correlation is really low), then I shouldn't average them together. So let's look real quick.

```{r, warning = FALSE}
cor.test(data3$Trust,data3$Rely, use = "pairwise.complete.obs")
```

Ok. They're correlated at about 0.80, which is not the best, but it does fall within the range that most consider appropriate for averaging. So, now I'll do that here.

```{r, warning = FALSE}
data3$trustRely <- (data3$Trust+data3$Rely)/2
```

Perfect! Now we're ready to explore and analyzing our data!
<br/><br/><br/><br/>




# Plotting
A central tenet of the theory I'm testing is that trust is not static across the contours of a relationship. In other words, people can trust and distrust the same person depending on the goal. If that's true, then we should be able to see it in our data. Below, I randomly select 4 participants and plot the trust they place in their best friend across their three most and least important goals

```{r, , echo=FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3.25,fig.show='hold',fig.align='center'}
# Set seed so everyone sees the same thing
set.seed(101)
# Sample 4 random PINS
byGoalSample<- sample(data3$PIN,4)
# Subset the data for the 4 random participants
byGoalSample2<-data3[data3$PIN %in% byGoalSample,]

byGoal3 <-ggplot(data = byGoalSample2, 
  aes(x = Goal, y=trustRely))+
  facet_grid(vars(PIN))+
  geom_point(aes(colour = PIN))+
  geom_line(aes(colour = PIN,group = 1))+
  xlab("Goal")+ylab("Trust")+ # add labels
  ylim(-4, 4)+
  theme_bw()+
  theme(panel.border = element_rect(fill = NA, colour = "NA"),
        axis.line = element_line(size = 1, colour = "grey80"),
        legend.position = "top",
        legend.title = element_blank())
byGoal3
```

B-e-a-utiful!! Whereas some people seem to be pretty consistent in their trust in their best friend, others appear to calibrate their trust in their friend across the goals in their lives.

Now that we've checked to see whether this was all probably a waste of time or whether there might be somemthing there, let's check out the other variables.
<br/><br/><br/>



## Histograms
Ok, now that we've centered our data, let's take a look at the histograms of our variables. First, let's look at the distribution of the relevance to strength measure.
<br/><br/><br/>



### Relevance to Strength
```{r, , echo=FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3.25,fig.show='hold',fig.align='center'}
rel_str_Hist <- data3 %>%
  ggplot(aes(x = StRel)) +
  geom_histogram() +
  xlab("Relevance to Strength")+ylab("Count") + # add labels 
  theme_bw() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_rect(fill = NA, colour = "NA"),
        axis.line = element_line(size = 1, colour = "grey80"),
        legend.position = "none",
        legend.title = element_blank())
rel_str_Hist
```

That loooks decent. It's a bit positively-skewed, which means that their are more values on the lower end of the scale than the higher end, but it doesn't look terrible. Now let's take a look at the relevance to weakness measure
<br/><br/>


### Relevance to Weakness
```{r, , echo=FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3.25,fig.show='hold',fig.align='center'}
rel_weak_Hist <- data3 %>%
  ggplot(aes(x = WeakRel)) +
  geom_histogram() +
  xlab("Relevance to Weakness")+ylab("Count") + # add labels 
  theme_bw() +
  theme(panel.grid.major = element_blank(),
       panel.grid.minor = element_blank(),
       panel.border = element_rect(fill = NA, colour = "NA"),
       axis.line = element_line(size = 1, colour = "grey80"),
       legend.position = "none",
       legend.title = element_blank())
rel_weak_Hist
```

That is *much* worse. Notice how nearly all of the values are at the lowest end of the scale? Not only is this extremely positively-skewed, it also looks like we're seeing a **floor effect**. A **Floor effect** is when the distribution of your variable is clustered at the lower bound of the variable range. This is not a good distribution, and it tells us that any potential failure to find an effect may be due to the hypothesis *or* the **floor effect**. That's not good, but it is what we have, so we'll see what we see.

Finally, let's check out the distribution of the continuous measure of goal importance.
<br/><br/>


### Goal Importance
```{r, , echo=FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3.25,fig.show='hold',fig.align='center'}
rel_weak_Hist <- data3 %>%
  ggplot(aes(x = Importance)) +
  geom_histogram() +
  xlab("Goal Importance")+ylab("Count") + # add labels 
  theme_bw() +
  theme(panel.grid.major = element_blank(),
       panel.grid.minor = element_blank(),
       panel.border = element_rect(fill = NA, colour = "NA"),
       axis.line = element_line(size = 1, colour = "grey80"),
       legend.position = "none",
       legend.title = element_blank())
rel_weak_Hist
```

In contrast to the last two histograms, this variable is a bit **negatively-skewed**, which means that most of the values are at the higher end of the scale. However, like the relevance to strength measure, this distribution is not worrisome. 

Now that we've checked the distributions of our predictor variables, let's plot out the relationships we plan to test.
<br/><br/><br/>



## Predicted Relationships
Before we plot the hypothesized relationships, let's first check to make sure that participants' most important goals were actually more important than their least important goals.
<br/><br/>


### Manipulation Check
```{r, , echo=FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3.25,fig.show='hold',fig.align='center'}
impCheck <- ggplot(data = data3,aes(x = Goal, y=ImportanceC))+
  geom_point(aes(colour = PIN))+
  geom_smooth(method = "lm", se = T, aes(group = impCat, colour=impCat))+
  xlab("Goal")+ylab("Importance")+ # add labels
  ylim(-4, 4)+#xlim(-4, 4)+
  theme_bw()+
  theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border = element_rect(fill = NA, colour = "NA"),
        axis.line = element_line(size = 1, colour = "grey80"),
        legend.position = "none",
        legend.title = element_blank())
impCheck
```

Perfect. Participants completed the study as expected! Now let's check out our hypotheses.

The first hypothesis was that participants would trust their best friend less for more important than less important goals. Let's see if that's what the relationship looks like.
<br/><br/>


### Trust By Importance
```{r, , echo=FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3.25,fig.show='hold',fig.align='center'}
# Graphing Trust by Importance
ImpTrust <- ggplot(data = data3, 
  aes(x = ImportanceC, y=trustRely))+
  #geom_point(aes(colour = PIN))+
  geom_smooth(method="lm", se = T)+#, aes(group = PIN))+# we add group level
  xlab("Importance")+ylab("Trust")+ # add labels
  ylim(-4, 4)+
  #xlim(-4, 4)+
  theme_bw()+
  theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border = element_rect(fill = NA, colour = "NA"),
        axis.line = element_line(size = 1, colour = "grey80"),
        legend.position = "none",
        legend.title = element_blank())
ImpTrust
```

Well, there doesn't appear to be much of a relationship there. In fact, it kinda looks like the slope is in the opposite direction than predicted. This is definitely something to look for in the stats we'll run in a bit.

The second hypothesis was that participants would trust their best friend *more* the more their friend's strength was relevant to the goal. Let's take a look.
<br/><br/>


### Trust By Strength
```{r, , echo=FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3.25,fig.show='hold',fig.align='center'}
relStr <- ggplot(data = data3, 
  aes(x = StrRelC, y=trustRely))+ 
  geom_smooth(method = "lm", se = T)+
  xlab("Relevance to Strength")+ylab("Trust")+ # add labels
  ylim(-4, 4)+
  theme_bw()+
  theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border = element_rect(fill = NA, colour = "NA"),
        axis.line = element_line(size = 1, colour = "grey80"),
        legend.position = "top",
        legend.title = element_blank())
relStr
```

That looks like a strong relationship in the direction we predicted! Yay!! 

The third prediction was that participants would trust their best friend *less* the more their friend's weakness was relevant to the goal.
<br/><br/>


### Trust By Weakness
```{r, , echo=FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3.25,fig.show='hold',fig.align='center'}
relWeak <- ggplot(data = data3, 
  aes(x = WeakRelC, y=trustRely))+ 
  geom_smooth(method = "lm", se = T)+
  xlab("Relevance to Weakness")+ylab("Trust")+ # add labels
  ylim(-4, 4)+
  theme_bw()+
  theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border = element_rect(fill = NA, colour = "NA"),
        axis.line = element_line(size = 1, colour = "grey80"),
        legend.position = "top",
        legend.title = element_blank())
relWeak
```

That looks like a weak relationship but again in the opposite direction than predicted. This is another finding we'll keep a sharp eye on in our analyses.

The final two hypotheses were that the relationships between the relevance to strength and the relevance to weakness on goal-specific trust would only show up for more important goals. There should be no relationship between the relevance to strength and the relevance to weakness on goal-specific trust for less important goals. Next I plot those predictions. We'll first take a look at the interaction between goal importance and the relevance to strength before examining the interaction between goal importance and the relevance to weakness.
<br/><br/>


### Interactions
```{r, , echo=FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3.25,fig.show='hold',fig.align='center'}
relStrImp <- ggplot(data = data3, 
  aes(x = StrRelC, y=trustRely))+ 
  geom_smooth(method = "lm", se = T, aes(group = impCat, colour=impCat))+ # add group level
  xlab("Relevance to Strength")+ylab("Trust")+ # add labels
  ylim(-4, 4)+
  theme_bw()+
  theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border = element_rect(fill = NA, colour = "NA"),
        axis.line = element_line(size = 1, colour = "grey80"),
        legend.position = "top",
        legend.title = element_blank())
relStrImp
```

```{r, , echo=FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3.25,fig.show='hold',fig.align='center'}
relWeakImp <- ggplot(data = data3, 
  aes(x = WeakRelC, y=trustRely))+ 
  geom_smooth(method = "lm", se = T, aes(group = impCat, colour=impCat))+# we add group level
  xlab("Relevance to Weakness")+ylab("Trust")+ # add labels
  ylim(-4, 4)+
  theme_bw()+
  theme(panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),
        panel.border = element_rect(fill = NA, colour = "NA"),
        axis.line = element_line(size = 1, colour = "grey80"),
        legend.position = "top",
        legend.title = element_blank())
relWeakImp
```

Welp. It looks like neither prediction was supported. Time to go run the statistical analyses to check what our graphs are telling us.
<br/><br/><br/><br/>




# Analyses
Now that we've graphed our data and gotten insight into our variables' distributions and the relationships between our predictor and criterion variables, it's time to run statistical analyses to formally test our hypotheses. 

The analyses that I will do below are linear multilevel (or mixed effects) models (MLMs). These analyses will allow me to model the random variance that is caused by our participants reporting different average levels of trust in their best friend (random intercept) and different relationships betweeen goal importance and goal-specific trust (random slope). 

As a side note, the most appropriate analyses for these data are actually ordinal logistic multilevel models. The criterion variable, goal-specific trust, is on an **ordinal scale**. Ordinal scales are where the distance between scale points are not even. By contrast, in **interval** or **ratio scales**, the distance between scale points is the same. An example of an interval scale is temperature in degrees Fahrenheit. The difference between 1$^\circ$ and 2$^\circ$ Fahrenheit is the same as the difference between 101$^\circ$ and 102$^\circ$ Fahrenheit. By contrast, is the difference between "slightly trust" and "moderately trust" the same as the difference between "moderately trust" and "very much trust?" I don't know, but probably not. Technically, linear MLMs require interval or ratio criterion variables whereas ordinal logistic MLMs require ordinal criterion variables. However, I ran both sets of analyses, and the conclusions remain the same. Consequently, I demonstrate linear MLMs here because they are generally easier to understand.

These analyses are a bit complicated and require us to carefully monitor changes in our output as a funciton of changes in our fixed and random effects. I will explain each step.
<br/><br/><br/>



## Random Effects
The first step in running MLMs is to examine your random structure. To do this, we will include only the criterion variable and an intercept in our model and systematically change our random effects. This part might be a bit easier to understand while doing it than me explaining it, so let's just dive right in.

In this first model, I am going to use what's called the maximal random structure. The maximal structure includes the random intercept *and* the random slope, and is our best model at reducing our Type I error rate (which is the probability of incorrectly saying there is a real effect when there is in fact no effect).

```{r}
max_random <- lmer(Importance ~ 1
         + (1+dummy(impCat)|PIN), data = data3, REML=F)
summary(max_random)
```

The first thing you should note is the warning that the model failed to converge. This means that the maximum likelihood estimation process (beyond the scope of this explanation) failed. A wise person once told me that a model that does not work cannot be the correct model. Although we're getting numbers here, it's probably best to reduce our random structure.  The second thing to note is that the correlaiton between the random intercept and the random slope is very high (-0.99). A perfect correlation (+/- 1.00) is very bad for our model, so that gives us further reason to reduce our random structure.

The very first change I like to make is to block the correlation between the random slope and intercept. This removes a single feature from our random effects, so it is a desirable first step. To do this, I'm just going to add another ```|``` in the random effects portion of the model, like this.

```{r}
no_cor_random <- lmer(Importance ~ 1
         + (1+dummy(impCat)||PIN), data = data3, REML=F)
summary(no_cor_random)
```

That worked! Notice how we didn't get anymore errors? This random structure does seem to fit our data a bit better. I also want you to notice that the residual variance increased ever so slightly. That's because we removed a random effect that was accounting for some of the variance (the correlation between the random slope and intercept). It's important to keep a close eye on your residuals as you explore different model fits.

Depending on the training you've received in MLMs, some would stop examining the random structure here. The argument goes that every reduction in the random structure has a potential (or inevitable) increase in the Type I error rate. However, I use parsimonious fitting (Bates, Kliegl, Vasishth, & Baayen, 2015) to systematically reduce the random terms and find the simplest model that does not sacrifice model fit. To do so, I will remove random effects one at a time and see the potential change in the residual of the model. The moment a model fits significantly worse, as determined by a Chi-square, I will stop and keep the more complex model. This process can be long, and it is indeed long for these data, so I will only show the first step.

In this first step, I am going to remove the random intercept. To execute this step, I simply need to change the "1" in my random effects portion to "0." Let's see what happens.

```{r}
no_intercept_random <- lmer(Importance ~ 1
         + (0+dummy(impCat)||PIN), data = data3, REML=F)
summary(no_intercept_random)
```

Notice that the residual variance increased by a lot?! That probably means this model is not as good a fit. Let's formally test our intuition.

```{r}
anova(no_cor_random, no_intercept_random)
```

As expected, the no intercepts model fits significantly worse. Normally, that would mean that I would keep the more complex model. However, when I added in the main effects, those models failed to converge with the more complex random structures. After hours of pouring through many many models, it turned out that the simplest model was the only one that converged across all analyses. Because you *must* have the same random structure to compare between models, that is the structure I will use here. The final random structure, then, only includes a random intercept.

Now that we've examined our random structure, let's start actually testing our hypotheses.
<br/><br/><br/>



## Fixed effects
In MLMs, fixed effects refer to the effects of interest. In other words, our fixed effects are the relationships between our predictor variables and our criterion variable. In the case of this study, our fixed effects are **goal importance**, **relevance to strength**, **relevance to weakness**, and the two-way interactions between **goal importance** and the two **relevance** variables. We will need four different models to examine all of our hypotheses, starting with a model that only includes the main effects of our predictor variables.

```{r}
main_effects <- lmer(trustRely ~ WeakRelC + StrRelC + impCat
                         + (1|PIN), data = data3, REML=F)
summary(main_effects)
```

What do we notice? First, our relevance to strength measure strongly and significantly predicts goal-specific trust. This was the effect we noticed in our graphs above and confirms both our hypothesis and intuitions from the graph. Second, and in line with the graphs but against our hypothesis, goal importance is a significant and *positive* predictor of trust, and the relevance to weakness is a marginally (ugh, I hate that word too) significant and *positive* predictor of trust. I'm not going to go through the implications for the theory here, so I encourage you to read my dissertation, if you're interested. :)

Next, let's add each of the interactions in one-at-a-time. We will test whether the more complex model fits the data better than the simpler model after each step

```{r}
## Adding interaction between Strength and Importance
strength_importance <- lmer(trustRely ~ WeakRelC + StrRelC*impCat
                                  + (1|PIN), data=data3, REML=F)
summary(strength_importance)
# The interaction is negative but not significant
anova(main_effects, strength_importance)
# Not a significant improvement in model fit
## Adding interaction between Weakness and Importance
weakness_importance<-lmer(trustRely ~ WeakRelC*impCat + StrRelC
                                  + (1|PIN), data=data3, REML=F)
summary(weakness_importance)
# The interaction is not significant
anova(main_effects, weakness_importance)
# Not a significant improvement in model fit
```

In contrast to our hypotheses, neither interaction is significant. However, we have one final model to test before we go home. We still need to test the model with both interactions included.

```{r}
## Adding both interactions for completeness
full_model <- lmer(trustRely ~ WeakRelC*impCat + StrRelC*impCat
                       + (1|PIN), data=data3, REML=F)
summary(full_model)
# Neither interaction is significant
anova(strength_importance, full_model)
anova(weakness_importance, full_model)
# Not a significant improvement in model fit over either singular interaction models
```

The full model is not an improvement over either single interaction models.
<br/><br/><br/><br/>




# Implications
So what did we learn? 
**First, trust varies within a relationship, and that variability is predictable.**

Second, the main effects only model was the best fit for the data. Consequently, in contrast to our hypotheses, neither of the interactions between **relevance** and **goal importance** predict goal-specific trust.

Third, **relevance to weakness** does not significantly predict trust, and **goal importance** positively predicts trust, both of which are counter to expectations. 

Fourth and finally, and in line with predictions, **relevance to strength** strongly and positively predicts trust. 
<br/><br/><br/><br/>




# Conclusion
I hope you have enjoyed and learned something from this overview and walkthrough of my dissertation logic and analyses! 
Please feel free to contact me if you're interested in trust, the stats presented here, or just to have a friendly chat!! I love meeting new people and nerding out on these topics.